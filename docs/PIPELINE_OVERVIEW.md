# Complete Data Processing Pipeline
## "Where to DINE" - Step-by-Step Workflow

**Purpose**: Visual guide to the entire data processing workflow from raw data to final recommendations

---

## ğŸ“Š Pipeline Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PHASE 0: SETUP                           â”‚
â”‚  â±ï¸ Time: 30 min | ğŸ’¾ Data: 0 GB                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Create directory structure                                   â”‚
â”‚  2. Install Python dependencies (pip install -r requirements.txt)â”‚
â”‚  3. Configure parameters (config/config.yaml)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1: DATA ACQUISITION                     â”‚
â”‚  â±ï¸ Time: 2-4 hours | ğŸ’¾ Data: ~70 GB                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: None (download from web)                                 â”‚
â”‚  SCRIPTS:                                                         â”‚
â”‚    - scripts/download_taxi_data.sh                               â”‚
â”‚    - scripts/download_gtfs_data.sh                               â”‚
â”‚    - (Restaurant data: already provided)                         â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/raw/taxi/*.parquet (12 files, ~50-70 GB)              â”‚
â”‚    âœ“ data/raw/gtfs/*.zip (7 files, ~100 MB)                     â”‚
â”‚    âœ“ data/raw/restaurants/*.csv (2 files, ~5 MB)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 2: TAXI DATA PREPROCESSING                    â”‚
â”‚  â±ï¸ Time: 30-60 min | ğŸ’¾ Input: 70 GB â†’ Output: 5 GB (90% â†“)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: data/raw/taxi/*.parquet                                  â”‚
â”‚  SCRIPT: src/data_processing/01_extract_taxi_data.py            â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load each month's Parquet file                             â”‚
â”‚    2. Temporal filter: Keep only dining hours                    â”‚
â”‚       (7-10 AM, 11-2 PM, 5-10 PM, 10 PM-1 AM)                   â”‚
â”‚    3. Spatial filter: Keep only NYC bounds                       â”‚
â”‚       (lat: 40.48-40.92, lon: -74.26 to -73.70)                 â”‚
â”‚    4. Quality filter: Remove nulls, invalid coordinates          â”‚
â”‚    5. Add temporal columns: hour, day_of_week                    â”‚
â”‚    6. Concatenate all months                                     â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/interim/taxi_filtered_dining_hours.parquet (~5 GB)    â”‚
â”‚    âœ“ ~20-30 million records (from 50M)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PHASE 3: RESTAURANT DATA PREPROCESSING                â”‚
â”‚  â±ï¸ Time: 5-10 min | ğŸ’¾ Input: 5 MB â†’ Output: 3 MB              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT:                                                           â”‚
â”‚    - data/raw/restaurants/restaurants_nyc_googlemaps.csv         â”‚
â”‚    - data/raw/restaurants/restaurants_nyc_osm.csv                â”‚
â”‚  SCRIPT: src/data_processing/02_merge_restaurants.py            â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load both CSV files                                        â”‚
â”‚    2. Standardize schemas (common columns)                       â”‚
â”‚    3. Convert to GeoDataFrame (lat/lon â†’ Point geometry)         â”‚
â”‚    4. Project to EPSG:2263 (meters for distance calculation)    â”‚
â”‚    5. Spatial deduplication:                                     â”‚
â”‚       - Find pairs within 50 meters                              â”‚
â”‚       - Calculate name similarity (Levenshtein distance)         â”‚
â”‚       - If distance < 50m AND similarity > 80%: merge            â”‚
â”‚    6. Transform back to WGS84                                    â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/interim/restaurants_merged.geojson (~18,500 records)  â”‚
â”‚    âœ“ data/interim/restaurants_merged.csv (for compatibility)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PHASE 4: GTFS DATA PREPROCESSING                   â”‚
â”‚  â±ï¸ Time: 2-5 min | ğŸ’¾ Input: 100 MB â†’ Output: 100 MB          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: data/raw/gtfs/*.zip                                      â”‚
â”‚  SCRIPT: src/data_processing/03_process_gtfs.py                 â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Unzip all GTFS files                                       â”‚
â”‚    2. Parse stops.txt â†’ GeoDataFrame                            â”‚
â”‚    3. Parse routes.txt                                           â”‚
â”‚    4. (Optional) Convert to network using peartree              â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/interim/gtfs_unpacked/ (directories)                  â”‚
â”‚    âœ“ data/processed/transit_stops_subway.geojson                â”‚
â”‚    âœ“ data/processed/transit_stops_bus.geojson                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             PHASE 5: OSM NETWORK EXTRACTION                      â”‚
â”‚  â±ï¸ Time: 10-20 min | ğŸ’¾ Output: 50-100 MB                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: None (download via osmnx API)                            â”‚
â”‚  SCRIPT: src/data_processing/04_build_osm_network.py            â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Define NYC bounding box                                    â”‚
â”‚    2. Download drive network (osmnx.graph_from_bbox)            â”‚
â”‚    3. Download walk network                                      â”‚
â”‚    4. Add travel time attributes to edges:                       â”‚
â”‚       time_min = (length_m / 1000) / speed_kmh Ã— 60             â”‚
â”‚    5. Save as GraphML                                            â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/processed/network_dataset/osm_drive_network.graphml   â”‚
â”‚    âœ“ data/processed/network_dataset/osm_walk_network.graphml    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PHASE 6: RESTAURANT CLUSTERING (HDBSCAN)                â”‚
â”‚  â±ï¸ Time: 5-10 min | ğŸ’¾ Output: 1-2 MB                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: data/interim/restaurants_merged.geojson                  â”‚
â”‚  SCRIPT: src/analysis/clustering.py (function: cluster_restaurants)â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load restaurant GeoDataFrame                               â”‚
â”‚    2. Project to EPSG:2263 (meters)                             â”‚
â”‚    3. Extract coordinates as numpy array                         â”‚
â”‚    4. Run HDBSCAN:                                               â”‚
â”‚       - min_cluster_size = 30                                    â”‚
â”‚       - min_samples = 10                                         â”‚
â”‚       - cluster_selection_epsilon = 200 (meters)                 â”‚
â”‚    5. Add cluster_id column                                      â”‚
â”‚    6. Calculate validation metrics (silhouette, Davies-Bouldin) â”‚
â”‚    7. Create cluster polygons (convex hull + 100m buffer)       â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/processed/dining_zones.geojson (~30 clusters)         â”‚
â”‚    âœ“ Validation metrics logged                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PHASE 7: TAXI DROP-OFF CLUSTERING (WITH WEIGHTING)          â”‚
â”‚  â±ï¸ Time: 15-30 min | ğŸ’¾ Input: 5 GB â†’ Output: 2 MB             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: data/interim/taxi_filtered_dining_hours.parquet          â”‚
â”‚  SCRIPT: src/analysis/clustering.py (function: cluster_taxi_dropoffs)â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load taxi data                                             â”‚
â”‚    2. Apply temporal weighting function w(t, d)                  â”‚
â”‚    3. Aggregate to H3 hexagons (resolution 10):                 â”‚
â”‚       - 50M points â†’ 500k hexagons (96% reduction!)             â”‚
â”‚    4. Project hexagon centroids to EPSG:2263                    â”‚
â”‚    5. Run HDBSCAN:                                               â”‚
â”‚       - min_cluster_size = 50                                    â”‚
â”‚       - min_samples = 15                                         â”‚
â”‚       - cluster_selection_epsilon = 250                          â”‚
â”‚    6. Calculate total weighted drops per cluster                â”‚
â”‚    7. Create cluster polygons (convex hull + 150m buffer)       â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/processed/taxi_hotspot_areas.geojson (~50 clusters)   â”‚
â”‚    âœ“ Each with 'total_weight' attribute                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PHASE 8: SPATIAL INTERSECTION (FINAL HOTSPOTS)           â”‚
â”‚  â±ï¸ Time: 1-2 min | ğŸ’¾ Output: 500 KB                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT:                                                           â”‚
â”‚    - data/processed/dining_zones.geojson                         â”‚
â”‚    - data/processed/taxi_hotspot_areas.geojson                   â”‚
â”‚  SCRIPT: src/analysis/hotspot_identification.py                 â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load both polygon layers                                   â”‚
â”‚    2. Nested loop: for each dining zone Ã— taxi zone:            â”‚
â”‚       a. Compute geometric intersection                          â”‚
â”‚       b. Skip if empty                                           â”‚
â”‚       c. Check filters:                                          â”‚
â”‚          - area(intersection) â‰¥ 10,000 mÂ²                       â”‚
â”‚          - overlap_ratio â‰¥ 0.15 (15%)                           â”‚
â”‚       d. If passes: add to final hotspots list                  â”‚
â”‚    3. Calculate composite scores:                                â”‚
â”‚       hotspot_score = 0.5 Ã— restaurant_score + 0.5 Ã— taxi_scoreâ”‚
â”‚    4. Normalize to [0, 100]                                      â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ data/processed/dining_hotspots_final.geojson (~47 hotspots)â”‚
â”‚    âœ“ Each with: num_restaurants, total_weight, hotspot_score    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 9: VALIDATION & ANALYSIS                      â”‚
â”‚  â±ï¸ Time: 10-20 min | ğŸ’¾ Output: Reports & figures              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: data/processed/dining_hotspots_final.geojson             â”‚
â”‚  SCRIPT: src/analysis/validation.py                             â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Ground truth comparison:                                   â”‚
â”‚       - Load known dining districts (Chinatown, Koreatown, etc.)â”‚
â”‚       - Calculate spatial overlap                                â”‚
â”‚       - Compute accuracy metrics                                 â”‚
â”‚    2. Statistical validation:                                    â”‚
â”‚       - Correlation tests (taxi vs. restaurant density)          â”‚
â”‚       - Clustering significance (Monte Carlo simulation)         â”‚
â”‚    3. Generate validation visualizations                         â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ outputs/reports/validation_metrics.json                     â”‚
â”‚    âœ“ outputs/figures/ground_truth_comparison.png                â”‚
â”‚    âœ“ outputs/figures/cluster_validation.png                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PHASE 10: VISUALIZATION & PRESENTATION                   â”‚
â”‚  â±ï¸ Time: 20-30 min | ğŸ’¾ Output: HTML maps, PNG figures         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT: All processed data                                       â”‚
â”‚  SCRIPTS:                                                         â”‚
â”‚    - src/visualization/maps.py                                   â”‚
â”‚    - src/visualization/plots.py                                  â”‚
â”‚    - notebooks/06_final_demo.ipynb                              â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Create interactive maps (Folium):                          â”‚
â”‚       - All hotspots with scores                                 â”‚
â”‚       - Clustering results comparison                            â”‚
â”‚       - Isochrone examples                                       â”‚
â”‚    2. Create static figures (Matplotlib):                        â”‚
â”‚       - Temporal distribution charts                             â”‚
â”‚       - Cluster validation plots                                 â”‚
â”‚       - Score distribution histograms                            â”‚
â”‚    3. Generate presentation demo                                 â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ outputs/maps/final_hotspots_map.html                       â”‚
â”‚    âœ“ outputs/maps/clustering_comparison.html                    â”‚
â”‚    âœ“ outputs/figures/*.png (publication-quality)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PHASE 11: RECOMMENDATION ENGINE (INTERACTIVE)              â”‚
â”‚  â±ï¸ Time: Real-time (2-5 sec per query) | ğŸ’¾ Output: JSON       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  INPUT:                                                           â”‚
â”‚    - User location (lat, lon)                                    â”‚
â”‚    - Mode (walk, drive, transit)                                 â”‚
â”‚    - Max time (minutes)                                          â”‚
â”‚  SCRIPT: src/analysis/recommendation.py                         â”‚
â”‚                                                                   â”‚
â”‚  OPERATIONS:                                                      â”‚
â”‚    1. Load network graph (for selected mode)                     â”‚
â”‚    2. Find nearest network node to user location                 â”‚
â”‚    3. Calculate isochrone (Dijkstra shortest path with cutoff)  â”‚
â”‚    4. Spatial query: hotspots intersecting isochrone             â”‚
â”‚    5. For each accessible hotspot:                               â”‚
â”‚       a. Calculate exact travel time                             â”‚
â”‚       b. Compute accessibility_score = 100 Ã— (1 - t/t_max)     â”‚
â”‚       c. Compute final_score = Î±Â·P + Î²Â·A                        â”‚
â”‚    6. Sort by final_score descending                             â”‚
â”‚    7. Return top K results                                       â”‚
â”‚                                                                   â”‚
â”‚  OUTPUT:                                                          â”‚
â”‚    âœ“ JSON with ranked hotspots                                   â”‚
â”‚    âœ“ Isochrone polygon (GeoJSON)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

## ğŸ“‹ Quick Reference: File Dependencies

### Input â†’ Script â†’ Output Chain

```
RAW DATA FILES
â”œâ”€â”€ taxi/*.parquet (70 GB)
â”œâ”€â”€ restaurants/*.csv (5 MB)
â””â”€â”€ gtfs/*.zip (100 MB)
       â†“
PROCESSING SCRIPTS (src/data_processing/)
â”œâ”€â”€ 01_extract_taxi_data.py
â”œâ”€â”€ 02_merge_restaurants.py
â”œâ”€â”€ 03_process_gtfs.py
â””â”€â”€ 04_build_osm_network.py
       â†“
INTERIM DATA FILES
â”œâ”€â”€ taxi_filtered_dining_hours.parquet (5 GB)
â”œâ”€â”€ restaurants_merged.geojson (3 MB)
â””â”€â”€ gtfs_unpacked/* (100 MB)
       â†“
ANALYSIS SCRIPTS (src/analysis/)
â”œâ”€â”€ clustering.py â†’ dining_zones.geojson, taxi_hotspot_areas.geojson
â”œâ”€â”€ hotspot_identification.py â†’ dining_hotspots_final.geojson
â”œâ”€â”€ network_analysis.py â†’ (loads networks)
â”œâ”€â”€ service_area.py â†’ (generates isochrones)
â””â”€â”€ recommendation.py â†’ (returns recommendations)
       â†“
FINAL OUTPUTS
â”œâ”€â”€ data/processed/dining_hotspots_final.geojson (500 KB)
â”œâ”€â”€ outputs/maps/*.html
â”œâ”€â”€ outputs/figures/*.png
â””â”€â”€ outputs/reports/*.json
```

---

## ğŸ¯ Execution Order: Master Script

**Option 1**: Run each script manually
```bash
# Phase 2
python src/data_processing/01_extract_taxi_data.py

# Phase 3
python src/data_processing/02_merge_restaurants.py

# Phase 4
python src/data_processing/03_process_gtfs.py

# Phase 5
python src/data_processing/04_build_osm_network.py

# Phase 6-8 (combined in one script)
python src/analysis/run_full_analysis.py

# Phase 9
python src/analysis/validation.py

# Phase 10
python src/visualization/generate_all_maps.py
```

**Option 2**: Run master script (I can create this)
```bash
python run_full_pipeline.py
```

---

## âš¡ Performance Summary

| Phase | Time | Bottleneck | Optimization |
|-------|------|------------|--------------|
| Data download | 2-4 hours | Network I/O | Use fast internet |
| Taxi filtering | 30-60 min | Disk I/O | Use SSD, Parquet format |
| Restaurant merge | 5-10 min | Spatial join | R-tree index |
| GTFS parsing | 2-5 min | Unzip | - |
| OSM download | 10-20 min | API rate limit | Retry logic |
| Restaurant clustering | 5-10 min | HDBSCAN | Already fast for 18k points |
| Taxi clustering | 15-30 min | HDBSCAN | **H3 aggregation (critical!)** |
| Intersection | 1-2 min | Geometry ops | R-tree index |
| Validation | 10-20 min | - | - |
| Visualization | 20-30 min | - | - |
| **TOTAL** | **4-6 hours** | - | - |

---

## ğŸ” Critical Decision Points

### Decision 1: Do you have the taxi data?
- **YES** â†’ Proceed with full pipeline
- **NO** â†’ Start with restaurant clustering only (Phase 6)

### Decision 2: Do you want to download OSM networks?
- **YES** â†’ Proceed with Phase 5
- **NO (use pre-computed)** â†’ Skip to Phase 6

### Decision 3: Do you want transit routing?
- **YES** â†’ Include Phase 4 (GTFS)
- **NO (walk + drive only)** â†’ Skip Phase 4, simpler implementation

### Decision 4: Do you want a web app?
- **YES** â†’ Add Phase 12 (Flask/FastAPI)
- **NO (Jupyter demo only)** â†’ Stop at Phase 10

---

## ğŸ“¦ What I Can Code For You Now

I can immediately write:

### **Essential Scripts** (Must Have):
1. âœ… `01_extract_taxi_data.py` - Taxi filtering
2. âœ… `02_merge_restaurants.py` - Restaurant deduplication
3. âœ… `03_process_gtfs.py` - GTFS parsing
4. âœ… `04_build_osm_network.py` - OSM download
5. âœ… `clustering.py` - HDBSCAN wrapper
6. âœ… `hotspot_identification.py` - Spatial intersection
7. âœ… `recommendation.py` - Recommendation engine

### **Utility Scripts** (Very Helpful):
8. âœ… `temporal_utils.py` - Weighting functions
9. âœ… `spatial_utils.py` - CRS transforms, distance calculations
10. âœ… `run_full_pipeline.py` - Master orchestration script

### **Analysis Scripts** (Nice to Have):
11. âœ… `validation.py` - Statistical validation
12. âœ… `generate_all_maps.py` - Visualization pipeline

### **Notebooks** (For Exploration):
13. âœ… `01_EDA_taxi_data.ipynb`
14. âœ… `02_EDA_restaurants.ipynb`
15. âœ… `03_clustering_experiments.ipynb`
16. âœ… `06_final_demo.ipynb`

---

## ğŸš€ Recommended Starting Point

### **If you have NO data yet**:
Start with: **Mock data testing**
- I'll create synthetic data generators
- Test the entire pipeline with fake data
- Verify code works before downloading 70 GB

### **If you have restaurant data only**:
Start with: **Phase 3 + Phase 6**
- Merge restaurants
- Cluster restaurants
- Visualize dining zones

### **If you have all data**:
Start with: **Full pipeline**
- I'll write all scripts in order
- We run each phase sequentially
- Debug as we go

---

## â“ Tell Me Your Situation

**Answer these questions:**

1. **Do you have the data?**
   - [ ] Yes, I have taxi data (all 12 months)
   - [ ] Yes, I have restaurant data
   - [ ] Yes, I have GTFS data
   - [ ] No, I have nothing yet

2. **What's your priority?**
   - [ ] Get something working ASAP (start small)
   - [ ] Build complete system (take time)
   - [ ] Just test concepts (mock data)

3. **What do you want first?**
   - [ ] Data processing scripts (Phase 2-5)
   - [ ] Analysis scripts (Phase 6-8)
   - [ ] Visualization (Phase 10)
   - [ ] Everything in order (Phase 2 â†’ Phase 11)

**Tell me and I'll start writing code immediately!** ğŸš€
